\documentclass{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}

\title{Dytype: a strongly normalizing, almost-dynamically-typed Lisp}
\author{Keith Pinson}

\begin{document}

\maketitle

\section{Introduction}

There is something picturesque about Lisp. Something with such a small core, yet
expandable in userspace to the standards of any other modern high-level
language. As illustrated by Joel Martin's ``Make a Lisp'', it is a great
training ground for building a programming language. Because of the small core,
in principle it is highly portable (think Shen). There's a magic in it having
just enough primitives---and yet, not so primitive, in that Lisp has introduced
a great number of the concepts that we now consider to be basic to high-level
languages.

This makes Lisp appealing for a use as a sort of portable scripting language, or
as a basis for fairly powerful domain specific languages, or perhaps, to dream a
little, as the kernel to bootstrap a language with a self-hosting type system.
In all such cases, however (and, if you find David Turner's work on total
functional programming compelling, in almost any case) one could wish to have a
Lisp that was strongly normalizing. This type of limitedness would fit well with
the smallness of Lisp's core.

The question becomes, can we implement a type system that will guarantee
termination, without losing minimalism, and without losing so much programming
power that the language becomes a curiosity rather than a useful tool? The Lisp
ecosystem has its interesting type systems. Racket has \texttt{\#lang
typed/racket}, which has its charms and warts, and is anything but minimalistic.
Shen has its exotic and powerful system, but it is overpowered for what we want
(as Deech has said, you can launch the missiles while type-checking),
and the types can be turned on and off. There is typed Scheme, and gradually
typed Clojure, and probably others; but none are concerned with termination, as
far as I'm aware.

To keep the system simple, we focus on termination as the only property we care
about. \texttt{(cdr \#t)} should not be caught at compile time by this system;
it can throw a runtime exception. Runtime exceptions of that sort are typically
easier to trace down that infinite loops; there is a quick feedback loop on
runtime type errors. Arguably, infinite recursion is an order of magnitude worse
problem than a runtime type error.

The most minimal type system that guarantees termination (perhaps the most
minimal interesting type system of all) is the simply typed lambda calculus. We
start with that, using only one base type, which we call 1 because using numbers
will provide a syntactic convenience. To this we add the ability to do primitive
mutual recursion on lists, without having to add a separate termination checker.
We also add a few touches to improve the fit with Lisp, such as variadic
function types.

\section{Typing rules}

For any primitive non-function literal $c$, we can synthesize its type as $1$
from the empty context:

\[ \frac{}{\vdash{}c\mathbin{\Rightarrow}1}\quad\text{(literal synthesis)} \]

Our only hope of synthesizing a lambda is to try and see if we can synthesize
something meaningful if we assign the parameter type $1$ (unless I read up more
on Hindley-Milner and better understand how to do this).

\[ \frac{\Gamma,x\mathbin{:}1\vdash e\mathbin{\Rightarrow}\alpha}{\Gamma\vdash
    (\lambda~(x)~e)\mathbin{\Rightarrow}1\to\alpha}\quad\text{(unary
    $\lambda$ synthesis)} \]

This can easily be extended to forms like $(\lambda~(x~y~z)~e)$ with
currying (or without, if we want to fully concede to Lisp).

Variadic functions can be recognized syntactically. Essentially, they just take
in the \textbf{cdr} when they are applied, which is just data, but needs a
sort of modal distinction, so

\[ \frac{\Gamma,x\mathbin{:}1\vdash e\mathbin{\Rightarrow}\alpha}{\Gamma\vdash
    (\lambda~x~e)\mathbin{\Rightarrow}\star\to\alpha}\quad\text{(variadic
    $\lambda$ synthesis)} \]

We have a bit of syntactic sugar for function types, based on reducing to
naturals numbers:

\[ 1\to1 = 2 \]
\[ 1\to2 = 3 \]

and so on. Only plain first-order functions can be reduced to a natural;
higher order functions reduce like so:

\[ (1\to{}1)\to{}1\to{}1 = 2\to{}2 \]

Notably, this does not reduce further:

\[ 2\to{}2 \not = 4 \]

and variadic functions don't reduce to a natural, either, e.g.:

\[ \star\to{}1\to{}1 =
  \star\to{}2 \]

A variadic function of type like
$1\to\star\to{}1$ does not reduce. Also, in
Lisp we don't have the nice infix arrow notation, so we actually write a type
like $2\to{}2$ as $(2~.~2)$ and $1\to\star\to{}1$ as $(1~.~*~.~1)$.

The application rules are mundane:

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}\alpha\to\beta\quad\Gamma\vdash
    x\mathbin{\Leftarrow}\alpha}{\Gamma\vdash
    (f~x)\mathbin{\Rightarrow}\beta}\quad\text{(Unary application synthesis)} \]

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}\star\to\alpha\quad\Gamma\vdash
    x\mathbin{\Leftarrow}1}{\Gamma\vdash
    (f~.~x)\mathbin{\Rightarrow}\alpha}\quad\text{(Variadic application synthesis)} \]

The second rule's syntax is imprecise: I do not mean that a literal dotted pair
is the syntax for invoking, but rather that the entire \textbf{cdr} is captured
and passed to the function. In other words, $(f~a~b~c)$ for the second rule is
treated like $(f~(\textbf{list}~a~b~c))$ would be for the first rule. This is a
concession to Lisp, rather than a particularly salient point of Dytype itself,
as I see it.

We adopt the standard rule that allows checking to be far more powerful via
synthesis:

\[ \frac{\Gamma\vdash e\mathbin{\Rightarrow}\alpha}{\Gamma\vdash
    e\mathbin{\Leftarrow}\alpha}\quad\text{(Synthesis suffices for checking)} \]

and the standard rule for looking up a variable in a context:

\[ \frac{x\mathbin{:}\alpha\in\Gamma}{\Gamma\vdash
    x\mathbin{\Rightarrow}\alpha}\quad\text{(Synthesis by lookup)} \]

And we allow for explicit annotations:

\[ \frac{\Gamma\vdash e\mathbin{\Leftarrow}\alpha}{\Gamma\vdash
    (: e~\alpha)\mathbin{\Rightarrow}\alpha}\quad\text{(Annotation synthesis)} \]

We do not want to implement a separate totality checker when the entire point of
the type-system is to provide totality. At the same time, the rules we have
provided do not offer any form of recursion whatsoever; the power of what is
already presented, aside from whatever we may decide to surface from the
underlying Scheme in the Prelude, is insufficient for almost any interesting
real coding example (fizzbuzz, anyone?). We want the language to be sufficiently
strong to typecheck itself; ideally, I would like to be able to do the Lisp
thing and write Dytype interpreter in Dytype (but am not sure if this will be
achievable). So we need rules for recursion, that allow us a sufficiently useful
amount of recursion, and yet rigorously enforce totality.

We propose a new binding form, $\mu$, that allows defining two mutually
recursive functions (how this can be simplified to the case of a self-recursive
function, or extended to a larger set of mutually recursive functions, or
extended to functions of multiple arguments, or extended to \textbf{cadr},
\textbf{cddr} and the like, or to numbers, is all contained by way of exemplar
in these rules). The first function, if it calls itself, must call itself back
with a substructure, and also if it calls the second. The second, if it calls
itself back, must use a substructure; but it may call the first with the same
size argument it received. The utility of this little extra bit of freedom is
seen when one tries to implement a typechecker that includes a rule like
``Synthesis suffices for checking'', above.

The key here is that recursive functions have a different type with respect to
themselves, internally, than they do with respect to the outside world; and
therefore, to invoke themselves they cannot employ the typical application
rules, but have to use specialized rules that enforce structural recursion.

\[
  \frac{
    \splitfrac{
      \splitfrac{
        \Gamma,x\mathbin{:}1,f_{1}\mathbin{:}1^{>}\to\alpha_{1},f_{2}\mathbin{:}1^{>}\to\alpha_{2}\vdash e_{1}\mathbin{\Leftarrow}\alpha_{1}
      }{
        \Gamma,x\mathbin{:}1,f_{1}\mathbin{:}1^{\geq}\to\alpha_{1},f_{2}\mathbin{:}1^{>}\to\alpha_{2}\vdash e_{2}\mathbin{\Leftarrow}\alpha_{2}
      }
    }{
      \Gamma,f_{1}\mathbin{:}1\to\alpha_{1},f_{2}\mathbin{:}1\to\alpha_{2}\vdash e_{3}\mathbin{\Rightarrow}\alpha_{3}
    }
  }{
    \Gamma\vdash
    (\mu~f_{1}~(x)~e_{1}~(1~.~\alpha_{1})~f_{2}~(x)~e_{2}~(1~.~\alpha_{2})~e_{3})\mathbin{\Rightarrow}\alpha_{3}
  }
\]

The expression is to be read as, ``define two mutually recursive functions,
$f_{1}$ and $f_{2}$, on recursive variable $x$, with function bodies $e_{1}$ and
$e_{2}$, respectively, and types $1\to\alpha_{1}$ and
$1\to\alpha_{2}$, respectively, such that they are bound in
expression $e_{3}$''. I have written this as a binding form, because the typing
rules are best expressed that way; in practice as implemented in a Scheme you
may want rather to do a conceptually similar thing as a top-level form, and
elaborate simply to \textbf{define}s.

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}1^{>}\to\alpha}{\Gamma\vdash
    (\downarrow~f~\textbf{car})\mathbin{\Rightarrow}\alpha} \]

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}1^{>}\to\alpha}{\Gamma\vdash
    (\downarrow~f~\textbf{cdr})\mathbin{\Rightarrow}\alpha} \]

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}1^{\geq}\to\alpha}{\Gamma\vdash
    (\downarrow~f~\textbf{car})\mathbin{\Rightarrow}\alpha} \]

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}1^{\geq}\to\alpha}{\Gamma\vdash
    (\downarrow~f~\textbf{cdr})\mathbin{\Rightarrow}\alpha} \]

\[ \frac{\Gamma\vdash f\mathbin{\Rightarrow}1^{\geq}\to\alpha}{\Gamma\vdash
    (\downarrow~f)\mathbin{\Rightarrow}\alpha} \]

The semantics of these calls are such that $(\downarrow~f~\textbf{car})$ would
translate to simply $(f~(\textbf{car}~x))$ in the Scheme backend, if $x$ was the
recursive variable. Note that the backend is then responsible for guarding
against shadowing of this variable, by performing any necessary
$\alpha$-conversions.

I do not have any formal proof (yet) of the soundness of this system, or its
termination. However, it appears to me to be simple enough so that with a little
reflection, it will be found convincing.

Something which has been given up here, is the ability to treat functions as
data. You cannot \textbf{cons} a function onto anything, nor can you
\textbf{cons} anything onto a function. This is because
$\textbf{cons}\mathbin{:}3$; i.e., on the one hand, it only takes in data, and
so cannot take in functions. On the other hand, if you were to be able to put a
function into a cons cell, the cons cell constructed would have type 1: i.e. it
would be a way of making the function type disappear, violating the fundamental
function/data distinction that makes it impossible to construct the type of
combinators like $Y$ or $\omega$. How important this limitation is, I am not yet
entirely sure, but I suspect it has the potential to be annoying. I do not yet
have a strategy for fixing this problem.

\end{document}

